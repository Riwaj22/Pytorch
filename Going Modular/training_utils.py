# -*- coding: utf-8 -*-
"""training_utils

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18qa2jn-hqtruz0VASslrP0YwhlwPr7n7
"""

import torch
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

# training_utils.py

from tqdm import tqdm
import torch

def train_step(model, data_loader, optimizer, loss_fn, accuracy_fn, device):
    model.train()
    total_loss, total_corrects = 0.0, 0

    for inputs, targets in data_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = loss_fn(outputs, targets)

        loss.backward()
        optimizer.step()

        total_loss += loss.item() * inputs.size(0)
        _, predictions = torch.max(outputs, 1)
        total_corrects += torch.sum(predictions == targets.data)

    average_loss = total_loss / len(data_loader.dataset)
    accuracy = total_corrects.double() / len(data_loader.dataset)

    return average_loss, accuracy

def test_step(model, data_loader, loss_fn, accuracy_fn, device):
    model.eval()
    total_loss, total_corrects = 0.0, 0

    with torch.inference_mode():
        for inputs, targets in data_loader:
            inputs, targets = inputs.to(device), targets.to(device)

            outputs = model(inputs)
            loss = loss_fn(outputs, targets)

            total_loss += loss.item() * inputs.size(0)
            _, predictions = torch.max(outputs, 1)
            total_corrects += torch.sum(predictions == targets.data)

    average_loss = total_loss / len(data_loader.dataset)
    accuracy = total_corrects.double() / len(data_loader.dataset)

    return average_loss, accuracy

def train(model, train_data_loader, test_data_loader, optimizer, loss_fn, accuracy_fn,
          epochs=5, early_stop_patience=3, device='cuda'):

    results = {
        'train_loss': [],
        'train_acc': [],
        'test_loss': [],
        'test_acc': []
    }

    best_test_loss = float('inf')
    patience_counter = 0

    for epoch in tqdm(range(epochs)):
        train_loss, train_acc = train_step(model, train_data_loader, optimizer, loss_fn, accuracy_fn, device)
        test_loss, test_acc = test_step(model, test_data_loader, loss_fn, accuracy_fn, device)

        print(f"Epoch:{epoch + 1} | Train Loss:{train_loss:.3f} | Train Accuracy:{train_acc:.3f} | Test Loss: {test_loss:.3f}| Test accuracy:{test_acc:.3f}")

        results['train_loss'].append(train_loss)
        results['train_acc'].append(train_acc)
        results['test_loss'].append(test_loss)
        results['test_acc'].append(test_acc)

        # Early stopping check
        if test_loss < best_test_loss:
            best_test_loss = test_loss
            patience_counter = 0
        else:
            patience_counter += 1

        if patience_counter >= early_stop_patience:
            print(f"Early stopping at epoch {epoch + 1} due to no improvement in test loss.")
            break

    return results

# # main_script.py

# from training_utils import train

# # Example usage
# # Assuming you have defined your model, train_data_loader, test_data_loader, optimizer, loss_fn, accuracy_fn, and device

# results = train(model_0, train_data_loader, test_data_loader, optimizer, loss_fn, accuracy_fn, epochs=100, early_stop_patience=10)
# print(results)